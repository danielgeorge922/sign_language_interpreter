{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa6b3f8-eb55-4484-97fb-5eca2069172f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Preprocessing function defined.\n",
      "MediaPipe setup completed.\n",
      "DataFrame initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(label_dir):\n\u001b[0;32m     57\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(label_dir, img_file)\n\u001b[1;32m---> 58\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     keypoints \u001b[38;5;241m=\u001b[39m extract_hand_keypoints(image)\n\u001b[0;32m     60\u001b[0m     row \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(keypoints, label)\n",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(image_path):\n\u001b[0;32m     20\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[1;32m---> 21\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(image, (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Data Collection and Preparation\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# Paths to the dataset directories\n",
    "train_dir = r'C:\\Users\\danie\\sign-language-alpha\\data\\asl_alphabet_train\\asl_alphabet_train'\n",
    "test_dir = r'C:\\Users\\danie\\sign-language-alpha\\data\\asl_alphabet_test\\asl_alphabet_test'\n",
    "\n",
    "# Step 1: Preprocessing Images\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(image, (64, 64))\n",
    "    return image\n",
    "\n",
    "print(\"Preprocessing function defined.\")\n",
    "\n",
    "# Step 2: Use MediaPipe for Hand Landmarks Extraction\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def extract_hand_keypoints(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks\n",
    "        keypoints = []\n",
    "        for handLms in hand_landmarks:\n",
    "            for lm in handLms.landmark:\n",
    "                keypoints.append([lm.x, lm.y, lm.z])\n",
    "        return np.array(keypoints).flatten()\n",
    "    return np.zeros(21 * 3 * 2)  # Return zero array if no hands detected\n",
    "\n",
    "print(\"MediaPipe setup completed.\")\n",
    "\n",
    "# Step 3: Create a DataFrame to Store Landmark Information\n",
    "columns = [f'x{i}' for i in range(21*2)] + [f'y{i}' for i in range(21*2)] + [f'z{i}' for i in range(21*2)] + ['label']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "print(\"DataFrame initialized.\")\n",
    "\n",
    "# Populate DataFrame\n",
    "rows = []\n",
    "for label in os.listdir(train_dir):\n",
    "    label_dir = os.path.join(train_dir, label)\n",
    "    for img_file in os.listdir(label_dir):\n",
    "        img_path = os.path.join(label_dir, img_file)\n",
    "        image = preprocess_image(img_path)\n",
    "        keypoints = extract_hand_keypoints(image)\n",
    "        row = np.append(keypoints, label)\n",
    "        rows.append(row)\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(rows, columns=columns)], ignore_index=True)\n",
    "\n",
    "print(\"DataFrame populated with landmark data.\")\n",
    "\n",
    "# Step 4: Model Training\n",
    "# Define Model Architecture\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(21*3*2,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(os.listdir(train_dir)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model architecture defined.\")\n",
    "\n",
    "# Prepare Training Data\n",
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].factorize()[0]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training and validation data prepared.\")\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "# Save the model and label encoder\n",
    "model.save('sign_language_model.h5')\n",
    "np.save('label_encoder_classes.npy', df['label'].factorize()[1])\n",
    "\n",
    "print(\"Model and label encoder saved.\")\n",
    "\n",
    "# Step 5: Live Translation Using OpenCV\n",
    "# Setup OpenCV for Real-Time Capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Starting real-time capture. Press 'q' to exit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "    keypoints = extract_hand_keypoints(frame)\n",
    "    keypoints = keypoints.reshape(1, -1)  # Reshape for the model input\n",
    "    \n",
    "    predictions = model.predict(keypoints)\n",
    "    predicted_label = df['label'].factorize()[1][np.argmax(predictions)]\n",
    "    print(f'Predicted Label: {predicted_label}')\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.putText(frame, f'Predicted: {predicted_label}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Real-time capture ended.\")\n",
    "\n",
    "# Step 6: Testing and Refinement\n",
    "# (This part is generally iterative and requires manual adjustments and testing in real-world scenarios)\n",
    "\n",
    "print(\"Testing and refinement phase. Evaluate the model's performance and make necessary adjustments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e992e87-412e-4668-b19e-09e369267d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
