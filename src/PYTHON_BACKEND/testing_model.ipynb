{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb73c6f0-0879-4e38-92a2-68adb749607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, \n",
    "                                     Flatten, Conv2D, GlobalAveragePooling2D,\n",
    "                                     MaxPooling2D, BatchNormalization)\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d070e46f-4737-4515-bbc4-0325d7831a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/danie/sign-language-alpha/data/asl_al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/danie/sign-language-alpha/data/asl_al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/danie/sign-language-alpha/data/asl_al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/danie/sign-language-alpha/data/asl_al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/danie/sign-language-alpha/data/asl_al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C:/Users/danie/sign-language-alpha/data/asl_al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C:/Users/danie/sign-language-alpha/data/asl_al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C:/Users/danie/sign-language-alpha/data/asl_al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C:/Users/danie/sign-language-alpha/data/asl_al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C:/Users/danie/sign-language-alpha/data/asl_al...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path label\n",
       "0  C:/Users/danie/sign-language-alpha/data/asl_al...     A\n",
       "1  C:/Users/danie/sign-language-alpha/data/asl_al...     A\n",
       "2  C:/Users/danie/sign-language-alpha/data/asl_al...     A\n",
       "3  C:/Users/danie/sign-language-alpha/data/asl_al...     A\n",
       "4  C:/Users/danie/sign-language-alpha/data/asl_al...     A\n",
       "5  C:/Users/danie/sign-language-alpha/data/asl_al...     A\n",
       "6  C:/Users/danie/sign-language-alpha/data/asl_al...     A\n",
       "7  C:/Users/danie/sign-language-alpha/data/asl_al...     A\n",
       "8  C:/Users/danie/sign-language-alpha/data/asl_al...     A\n",
       "9  C:/Users/danie/sign-language-alpha/data/asl_al...     A"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_dir = 'C:/Users/danie/sign-language-alpha/data/asl_alphabet_train/asl_alphabet_train'\n",
    "\n",
    "def generate_dataset(path):\n",
    "    data = {'path': [], 'label': []}\n",
    "    \n",
    "    folders = os.listdir(path)\n",
    "    \n",
    "    for folder in folders:\n",
    "        folderpath = os.path.join(path, folder)\n",
    "        files = os.listdir(folderpath)\n",
    "        \n",
    "        for file in files:\n",
    "            filepath = os.path.join(folderpath, file)\n",
    "            \n",
    "            data['path'].append(filepath)\n",
    "            data['label'].append(folder)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = generate_dataset(train_data_dir)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c311fe1-d95e-4378-b89f-6da96e4ccb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_splitter(dataset, train_size = 0.9, shuffle = True, random_state = 0):\n",
    "    train_df, val_df = train_test_split(dataset, train_size = train_size, shuffle = shuffle, random_state = random_state)\n",
    "\n",
    "    train_df = train_df.reset_index(drop = True)\n",
    "    val_df = val_df.reset_index(drop = True)\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "train_df, val_df = dataset_splitter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51178586-a668-4c78-852d-6fca3ca74d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74798 validated image filenames belonging to 29 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:920: UserWarning: Found 1 invalid image filename(s) in x_col=\"path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8312 validated image filenames belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "IMG_SIZE = (64, 64)\n",
    "\n",
    "train_generator = ImageDataGenerator(1./255, \n",
    "                                     rotation_range = 10, \n",
    "                                     height_shift_range = 0.1, \n",
    "                                     width_shift_range = 0.1)\n",
    "\n",
    "val_generator = ImageDataGenerator(1./255)\n",
    "\n",
    "train_images = train_generator.flow_from_dataframe(train_df, x_col = 'path', y_col = 'label', \n",
    "                                                   color_mode = 'grayscale', class_mode = 'categorical',\n",
    "                                                   batch_size = BATCH_SIZE, target_size = IMG_SIZE, \n",
    "                                                   shuffle = True, seed = 0)\n",
    "\n",
    "val_images = val_generator.flow_from_dataframe(val_df, x_col = 'path', y_col = 'label', \n",
    "                                               color_mode = 'grayscale', class_mode = 'categorical', \n",
    "                                               batch_size = BATCH_SIZE, target_size = IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25cec09c-42db-4d03-beb8-b6bf1b29bdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"FINAL.h5\")\n",
    "\n",
    "class_mapping = train_images.class_indices\n",
    "\n",
    "def get_class_label(predictions, class_mapping):\n",
    "    labels_mapping = {v: k for k, v in class_mapping.items()}\n",
    "    predicted_labels = [labels_mapping[pred] for pred in predictions]\n",
    "    return predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ca2f7a-0b3e-4c45-956b-2eeb9e9a2093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74798 validated image filenames belonging to 29 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:920: UserWarning: Found 1 invalid image filename(s) in x_col=\"path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels A_test.jpg -----> ['A']\n",
      "Predicted Labels B_test.jpg -----> ['B']\n",
      "Predicted Labels C_test.jpg -----> ['C']\n",
      "Predicted Labels D_test.jpg -----> ['D']\n",
      "Predicted Labels E_test.jpg -----> ['E']\n",
      "Predicted Labels F_test.jpg -----> ['F']\n",
      "Predicted Labels G_test.jpg -----> ['G']\n",
      "Predicted Labels H_test.jpg -----> ['H']\n",
      "Predicted Labels I_test.jpg -----> ['I']\n",
      "Predicted Labels J_test.jpg -----> ['J']\n",
      "Predicted Labels K_test.jpg -----> ['K']\n",
      "Predicted Labels L_test.jpg -----> ['L']\n",
      "Predicted Labels M_test.jpg -----> ['M']\n",
      "Predicted Labels nothing_test.jpg -----> ['nothing']\n",
      "Predicted Labels N_test.jpg -----> ['N']\n",
      "Predicted Labels O_test.jpg -----> ['O']\n",
      "Predicted Labels P_test.jpg -----> ['P']\n",
      "Predicted Labels Q_test.jpg -----> ['Q']\n",
      "Predicted Labels R_test.jpg -----> ['R']\n",
      "Predicted Labels space_test.jpg -----> ['space']\n",
      "Predicted Labels S_test.jpg -----> ['S']\n",
      "Predicted Labels T_test.jpg -----> ['T']\n",
      "Predicted Labels U_test.jpg -----> ['U']\n",
      "Predicted Labels V_test.jpg -----> ['V']\n",
      "Predicted Labels W_test.jpg -----> ['W']\n",
      "Predicted Labels X_test.jpg -----> ['X']\n",
      "Predicted Labels Y_test.jpg -----> ['J']\n",
      "Predicted Labels Z_test.jpg -----> ['Z']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_images = train_generator.flow_from_dataframe(train_df, x_col = 'path', y_col = 'label', \n",
    "                                                   color_mode = 'grayscale', class_mode = 'categorical',\n",
    "                                                   batch_size = BATCH_SIZE, target_size = IMG_SIZE, \n",
    "                                                   shuffle = True, seed = 0)\n",
    "\n",
    "imgs_dir = r'C:\\Users\\danie\\sign-language-alpha\\data\\asl_alphabet_test\\asl_alphabet_test'\n",
    "imgs = os.listdir(imgs_dir)\n",
    "\n",
    "class_mapping = train_images.class_indices\n",
    "\n",
    "def get_class_label(predictions, class_mapping):\n",
    "    labels_mapping = {v: k for k, v in class_mapping.items()}\n",
    "    predicted_labels = [labels_mapping[pred] for pred in predictions]\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "def predict_image_class(model, img_path, class_mapping):\n",
    "    img = cv2.imread(os.path.join(imgs_dir, img_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(cv2.resize(img, (64, 64)), axis = 0)\n",
    "    predictions = np.argmax(model.predict(img, verbose = 0), axis = 1)\n",
    "    predicted_labels = get_class_label(predictions, class_mapping)\n",
    "    return predicted_labels\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def process_and_save_video_frame(frame, save_path=\"processed_frame.jpg\"):\n",
    "    \"\"\"\n",
    "    Process a video frame: convert to grayscale, resize with aspect ratio,\n",
    "    normalize, reshape, and save the processed frame.\n",
    "    \n",
    "    Args:\n",
    "        frame: The original video frame captured from the webcam.\n",
    "        save_path: The path to save the processed frame image.\n",
    "    \n",
    "    Returns:\n",
    "        frame_final: The processed frame ready for model input.\n",
    "    \"\"\"\n",
    "    def resize_with_aspect_ratio(image, target_size):\n",
    "        h, w = image.shape[:2]\n",
    "        target_w, target_h = target_size\n",
    "        scale = min(target_w / w, target_h / h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        resized = cv2.resize(image, (new_w, new_h))\n",
    "        delta_w, delta_h = target_w - new_w, target_h - new_h\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "        color = [0, 0, 0]\n",
    "        new_image = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "        return new_image\n",
    "\n",
    "    print(f\"Original Frame shape: {frame.shape}\")\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize with aspect ratio\n",
    "    frame_resized = resize_with_aspect_ratio(frame_gray, (64, 64))\n",
    "    print(f\"Shape after resizing with aspect ratio to 64x64: {frame_resized.shape}\")\n",
    "    \n",
    "    # Normalize\n",
    "    frame_normalized = frame_resized.astype('float32') / 255.0\n",
    "    \n",
    "    # Add batch and channel dimensions\n",
    "    frame_final = np.expand_dims(frame_normalized, axis=0)\n",
    "    frame_final = np.expand_dims(frame_final, axis=-1)\n",
    "    print(f\"Shape after adding batch and channel dimensions: {frame_final.shape}\")\n",
    "    \n",
    "    # Save the processed frame\n",
    "    cv2.imwrite(save_path, frame_resized * 255)\n",
    "    \n",
    "    return frame_final\n",
    "\n",
    "\n",
    "for img in imgs:\n",
    "    predicted_labels = predict_image_class(model, img, class_mapping)\n",
    "    print(f'Predicted Labels {img} -----> {predicted_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5cd5338-f96b-4d7d-906c-6c4fa135f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels A_test.jpg -----> ['A']\n",
      "Predicted Labels B_test.jpg -----> ['B']\n",
      "Predicted Labels C_test.jpg -----> ['C']\n",
      "Predicted Labels D_test.jpg -----> ['D']\n",
      "Predicted Labels E_test.jpg -----> ['E']\n",
      "Predicted Labels F_test.jpg -----> ['F']\n",
      "Predicted Labels G_test.jpg -----> ['G']\n",
      "Predicted Labels H_test.jpg -----> ['H']\n",
      "Predicted Labels I_test.jpg -----> ['I']\n",
      "Predicted Labels J_test.jpg -----> ['J']\n",
      "Predicted Labels K_test.jpg -----> ['K']\n",
      "Predicted Labels L_test.jpg -----> ['L']\n",
      "Predicted Labels M_test.jpg -----> ['M']\n",
      "Predicted Labels nothing_test.jpg -----> ['nothing']\n",
      "Predicted Labels N_test.jpg -----> ['N']\n",
      "Predicted Labels O_test.jpg -----> ['O']\n",
      "Predicted Labels P_test.jpg -----> ['P']\n",
      "Predicted Labels Q_test.jpg -----> ['Q']\n",
      "Predicted Labels R_test.jpg -----> ['R']\n",
      "Predicted Labels space_test.jpg -----> ['space']\n",
      "Predicted Labels S_test.jpg -----> ['S']\n",
      "Predicted Labels T_test.jpg -----> ['T']\n",
      "Predicted Labels U_test.jpg -----> ['U']\n",
      "Predicted Labels V_test.jpg -----> ['V']\n",
      "Predicted Labels W_test.jpg -----> ['W']\n",
      "Predicted Labels X_test.jpg -----> ['X']\n",
      "Predicted Labels Y_test.jpg -----> ['J']\n",
      "Predicted Labels Z_test.jpg -----> ['Z']\n"
     ]
    }
   ],
   "source": [
    "imgs_dir = r'C:\\Users\\danie\\sign-language-alpha\\data\\asl_alphabet_test\\asl_alphabet_test'\n",
    "imgs = os.listdir(imgs_dir)\n",
    "\n",
    "class_mapping = train_images.class_indices\n",
    "\n",
    "def get_class_label(predictions, class_mapping):\n",
    "    labels_mapping = {v: k for k, v in class_mapping.items()}\n",
    "    predicted_labels = [labels_mapping[pred] for pred in predictions]\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "def predict_image_class(model, img_path, class_mapping):\n",
    "    img = cv2.imread(os.path.join(imgs_dir, img_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(cv2.resize(img, (64, 64)), axis = 0)\n",
    "    predictions = np.argmax(model.predict(img, verbose = 0), axis = 1)\n",
    "    predicted_labels = get_class_label(predictions, class_mapping)\n",
    "    return predicted_labels\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def process_and_save_video_frame(frame, save_path=\"processed_frame.jpg\"):\n",
    "    \"\"\"\n",
    "    Process a video frame: convert to grayscale, resize with aspect ratio,\n",
    "    normalize, reshape, and save the processed frame.\n",
    "    \n",
    "    Args:\n",
    "        frame: The original video frame captured from the webcam.\n",
    "        save_path: The path to save the processed frame image.\n",
    "    \n",
    "    Returns:\n",
    "        frame_final: The processed frame ready for model input.\n",
    "    \"\"\"\n",
    "    def resize_with_aspect_ratio(image, target_size):\n",
    "        h, w = image.shape[:2]\n",
    "        target_w, target_h = target_size\n",
    "        scale = min(target_w / w, target_h / h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        resized = cv2.resize(image, (new_w, new_h))\n",
    "        delta_w, delta_h = target_w - new_w, target_h - new_h\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "        color = [0, 0, 0]\n",
    "        new_image = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "        return new_image\n",
    "\n",
    "    print(f\"Original Frame shape: {frame.shape}\")\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize with aspect ratio\n",
    "    frame_resized = resize_with_aspect_ratio(frame_gray, (64, 64))\n",
    "    print(f\"Shape after resizing with aspect ratio to 64x64: {frame_resized.shape}\")\n",
    "    \n",
    "    # Normalize\n",
    "    frame_normalized = frame_resized.astype('float32') / 255.0\n",
    "    \n",
    "    # Add batch and channel dimensions\n",
    "    frame_final = np.expand_dims(frame_normalized, axis=0)\n",
    "    frame_final = np.expand_dims(frame_final, axis=-1)\n",
    "    print(f\"Shape after adding batch and channel dimensions: {frame_final.shape}\")\n",
    "    \n",
    "    # Save the processed frame\n",
    "    cv2.imwrite(save_path, frame_resized * 255)\n",
    "    \n",
    "    return frame_final\n",
    "\n",
    "\n",
    "for img in imgs:\n",
    "    predicted_labels = predict_image_class(model, img, class_mapping)\n",
    "    print(f'Predicted Labels {img} -----> {predicted_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f9f39f9-add0-4e2e-8978-85e0d1deb1df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74798 validated image filenames belonging to 29 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:920: UserWarning: Found 1 invalid image filename(s) in x_col=\"path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8312 validated image filenames belonging to 29 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Constants\n",
    "train_data_dir = 'C:/Users/danie/sign-language-alpha/data/asl_alphabet_train/asl_alphabet_train'\n",
    "BATCH_SIZE = 128\n",
    "IMG_SIZE = (64, 64)\n",
    "\n",
    "# Generate dataset function\n",
    "def generate_dataset(path):\n",
    "    data = {'path': [], 'label': []}\n",
    "    folders = os.listdir(path)\n",
    "    for folder in folders:\n",
    "        folderpath = os.path.join(path, folder)\n",
    "        files = os.listdir(folderpath)\n",
    "        for file in files:\n",
    "            filepath = os.path.join(folderpath, file)\n",
    "            data['path'].append(filepath)\n",
    "            data['label'].append(folder)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Split dataset function\n",
    "def dataset_splitter(dataset, train_size=0.9, shuffle=True, random_state=0):\n",
    "    train_df, val_df = train_test_split(dataset, train_size=train_size, shuffle=shuffle, random_state=random_state)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    return train_df, val_df\n",
    "\n",
    "# Generate and split dataset\n",
    "df = generate_dataset(train_data_dir)\n",
    "train_df, val_df = dataset_splitter(df)\n",
    "\n",
    "# Image data generators\n",
    "train_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                     rotation_range=10, \n",
    "                                     height_shift_range=0.1, \n",
    "                                     width_shift_range=0.1)\n",
    "val_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_images = train_generator.flow_from_dataframe(train_df, x_col='path', y_col='label', \n",
    "                                                   color_mode='grayscale', class_mode='categorical',\n",
    "                                                   batch_size=BATCH_SIZE, target_size=IMG_SIZE, \n",
    "                                                   shuffle=True, seed=0)\n",
    "\n",
    "val_images = val_generator.flow_from_dataframe(val_df, x_col='path', y_col='label', \n",
    "                                               color_mode='grayscale', class_mode='categorical', \n",
    "                                               batch_size=BATCH_SIZE, target_size=IMG_SIZE)\n",
    "\n",
    "# Load model\n",
    "model = load_model(\"FINAL.h5\")\n",
    "\n",
    "# Class mapping\n",
    "class_mapping = train_images.class_indices\n",
    "\n",
    "# Get class label from predictions\n",
    "def get_class_label(predictions, class_mapping):\n",
    "    labels_mapping = {v: k for k, v in class_mapping.items()}\n",
    "    predicted_labels = [labels_mapping[pred] for pred in predictions]\n",
    "    return predicted_labels\n",
    "\n",
    "# Predict image class\n",
    "def predict_image_class(model, img, class_mapping):\n",
    "    if img is None:\n",
    "        print(\"Error: Unable to load image.\")\n",
    "        return [\"Error\"]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    img = img.astype('float32') / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    predictions = np.argmax(model.predict(img, verbose=0), axis=1)\n",
    "    return get_class_label(predictions, class_mapping)\n",
    "\n",
    "# Process video frame\n",
    "def process_frame(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = cv2.resize(frame, (64, 64))\n",
    "    frame = frame.astype('float32') / 255.0\n",
    "    frame = np.expand_dims(frame, axis=0)\n",
    "    frame = np.expand_dims(frame, axis=-1)\n",
    "    return frame\n",
    "\n",
    "# Predict image class from frame\n",
    "def predict_image_class_from_frame(model, frame, class_mapping):\n",
    "    processed_frame = process_frame(frame)\n",
    "    predictions = np.argmax(model.predict(processed_frame, verbose=0), axis=1)\n",
    "    return get_class_label(predictions, class_mapping)\n",
    "\n",
    "# Main script to capture video and make predictions\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    predicted_labels = predict_image_class_from_frame(model, frame, class_mapping)\n",
    "    \n",
    "    # Save the frame for debugging\n",
    "    frame_filename = f'debug_frame_{frame_count}.png'\n",
    "    cv2.imwrite(frame_filename, frame)\n",
    "    \n",
    "    # Load the saved frame to verify preprocessing\n",
    "    loaded_frame = cv2.imread(frame_filename)\n",
    "    if loaded_frame is None:\n",
    "        print(f\"Error: Unable to load saved frame {frame_filename}\")\n",
    "        continue\n",
    "    \n",
    "    # Process the loaded frame\n",
    "    processed_loaded_frame = process_frame(loaded_frame)\n",
    "    \n",
    "    # Ensure the loaded frame and original frame preprocessing are consistent\n",
    "    assert np.array_equal(processed_loaded_frame, process_frame(frame)), \"Mismatch in frame preprocessing\"\n",
    "\n",
    "    # Display the predicted label on the frame\n",
    "    cv2.putText(frame, predicted_labels[0], (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Sign Language Prediction', frame)\n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab61eb4-e350-4fe1-838b-134aa90563a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original frame shape: (480, 640, 3)\n",
      "Saved frame shape: (480, 640, 3)\n",
      "Processed original frame shape: (1, 64, 64, 1)\n",
      "Processed saved frame shape: (1, 64, 64, 1)\n",
      "There is a mismatch between the processed original frame and saved frame.\n"
     ]
    }
   ],
   "source": [
    "def examine_image_shapes(original_frame, saved_frame_path):\n",
    "    saved_frame = cv2.imread(saved_frame_path)\n",
    "    \n",
    "    if saved_frame is None:\n",
    "        print(f\"Error: Unable to load saved frame {saved_frame_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Original frame shape: {original_frame.shape}\")\n",
    "    print(f\"Saved frame shape: {saved_frame.shape}\")\n",
    "    \n",
    "    processed_original = process_frame(original_frame)\n",
    "    processed_saved = process_frame(saved_frame)\n",
    "    \n",
    "    print(f\"Processed original frame shape: {processed_original.shape}\")\n",
    "    print(f\"Processed saved frame shape: {processed_saved.shape}\")\n",
    "    \n",
    "    if np.array_equal(processed_original, processed_saved):\n",
    "        print(\"The processed original frame and saved frame are identical.\")\n",
    "    else:\n",
    "        print(\"There is a mismatch between the processed original frame and saved frame.\")\n",
    "\n",
    "# Test the shape examination function with an example frame\n",
    "test_frame = cv2.imread('debug_frame_16.png')  # Example frame captured earlier\n",
    "examine_image_shapes(frame, 'debug_frame_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ade13861-75e4-4056-95da-d96428f06edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels C:\\Users\\danie\\sign-langauge-translator\\src\\PYTHON_BACKEND\\debug_frame_91.png -----> ['O']\n"
     ]
    }
   ],
   "source": [
    "imgs_dir = r'C:\\Users\\danie\\sign-language-alpha\\data\\asl_alphabet_test\\asl_alphabet_test'\n",
    "imgs = os.listdir(imgs_dir)\n",
    "\n",
    "class_mapping = train_images.class_indices\n",
    "\n",
    "def get_class_label(predictions, class_mapping):\n",
    "    labels_mapping = {v: k for k, v in class_mapping.items()}\n",
    "    predicted_labels = [labels_mapping[pred] for pred in predictions]\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "def predict_image_class(model, img_path, class_mapping):\n",
    "    img = cv2.imread(os.path.join(imgs_dir, img_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = np.expand_dims(cv2.resize(img, (64, 64)), axis = 0)\n",
    "    predictions = np.argmax(model.predict(img, verbose = 0), axis = 1)\n",
    "    predicted_labels = get_class_label(predictions, class_mapping)\n",
    "    return predicted_labels\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def process_and_save_video_frame(frame, save_path=\"processed_frame.jpg\"):\n",
    "    \"\"\"\n",
    "    Process a video frame: convert to grayscale, resize with aspect ratio,\n",
    "    normalize, reshape, and save the processed frame.\n",
    "    \n",
    "    Args:\n",
    "        frame: The original video frame captured from the webcam.\n",
    "        save_path: The path to save the processed frame image.\n",
    "    \n",
    "    Returns:\n",
    "        frame_final: The processed frame ready for model input.\n",
    "    \"\"\"\n",
    "    def resize_with_aspect_ratio(image, target_size):\n",
    "        h, w = image.shape[:2]\n",
    "        target_w, target_h = target_size\n",
    "        scale = min(target_w / w, target_h / h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        resized = cv2.resize(image, (new_w, new_h))\n",
    "        delta_w, delta_h = target_w - new_w, target_h - new_h\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "        color = [0, 0, 0]\n",
    "        new_image = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "        return new_image\n",
    "\n",
    "    print(f\"Original Frame shape: {frame.shape}\")\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize with aspect ratio\n",
    "    frame_resized = resize_with_aspect_ratio(frame_gray, (64, 64))\n",
    "    print(f\"Shape after resizing with aspect ratio to 64x64: {frame_resized.shape}\")\n",
    "    \n",
    "    # Normalize\n",
    "    frame_normalized = frame_resized.astype('float32') / 255.0\n",
    "    \n",
    "    # Add batch and channel dimensions\n",
    "    frame_final = np.expand_dims(frame_normalized, axis=0)\n",
    "    frame_final = np.expand_dims(frame_final, axis=-1)\n",
    "    print(f\"Shape after adding batch and channel dimensions: {frame_final.shape}\")\n",
    "    \n",
    "    # Save the processed frame\n",
    "    cv2.imwrite(save_path, frame_resized * 255)\n",
    "    \n",
    "    return frame_final\n",
    "\n",
    "img = r'C:\\Users\\danie\\sign-langauge-translator\\src\\PYTHON_BACKEND\\debug_frame_91.png'\n",
    "\n",
    "predicted_labels = predict_image_class(model, img, class_mapping)\n",
    "print(f'Predicted Labels {img} -----> {predicted_labels}')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2f6e4fb-7688-457d-b91e-8f4069ace161",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'path_to_your_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load your model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_to_your_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Assume `train_images` is available and has `class_indices` attribute\u001b[39;00m\n\u001b[0;32m     10\u001b[0m class_mapping \u001b[38;5;241m=\u001b[39m train_images\u001b[38;5;241m.\u001b[39mclass_indices\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:189\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    183\u001b[0m         filepath,\n\u001b[0;32m    184\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    186\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:116\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    114\u001b[0m opened_new_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opened_new_file:\n\u001b[1;32m--> 116\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     f \u001b[38;5;241m=\u001b[39m filepath\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\h5py\\_hl\\files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'path_to_your_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your model\n",
    "model = load_model('path_to_your_model.h5')\n",
    "\n",
    "# Assume `train_images` is available and has `class_indices` attribute\n",
    "class_mapping = train_images.class_indices\n",
    "\n",
    "def get_class_label(predictions, class_mapping):\n",
    "    labels_mapping = {v: k for k, v in class_mapping.items()}\n",
    "    predicted_labels = [labels_mapping[pred] for pred in predictions]\n",
    "    return predicted_labels\n",
    "\n",
    "def predict_image_class(model, frame, class_mapping):\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame_resized = cv2.resize(frame_gray, (64, 64))\n",
    "    frame_expanded = np.expand_dims(frame_resized, axis=(0, -1))\n",
    "    frame_normalized = frame_expanded.astype('float32') / 255.0\n",
    "    predictions = np.argmax(model.predict(frame_normalized, verbose=0), axis=1)\n",
    "    predicted_labels = get_class_label(predictions, class_mapping)\n",
    "    return predicted_labels\n",
    "\n",
    "def process_and_save_video_frame(frame, save_path=\"processed_frame.jpg\"):\n",
    "    def resize_with_aspect_ratio(image, target_size):\n",
    "        h, w = image.shape[:2]\n",
    "        target_w, target_h = target_size\n",
    "        scale = min(target_w / w, target_h / h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        resized = cv2.resize(image, (new_w, new_h))\n",
    "        delta_w, delta_h = target_w - new_w, target_h - new_h\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "        color = [0, 0, 0]\n",
    "        new_image = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "        return new_image\n",
    "\n",
    "    print(f\"Original Frame shape: {frame.shape}\")\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize with aspect ratio\n",
    "    frame_resized = resize_with_aspect_ratio(frame_gray, (64, 64))\n",
    "    print(f\"Shape after resizing with aspect ratio to 64x64: {frame_resized.shape}\")\n",
    "    \n",
    "    # Normalize\n",
    "    frame_normalized = frame_resized.astype('float32') / 255.0\n",
    "    \n",
    "    # Add batch and channel dimensions\n",
    "    frame_final = np.expand_dims(frame_normalized, axis=0)\n",
    "    frame_final = np.expand_dims(frame_final, axis=-1)\n",
    "    print(f\"Shape after adding batch and channel dimensions: {frame_final.shape}\")\n",
    "    \n",
    "    # Save the processed frame\n",
    "    cv2.imwrite(save_path, frame_resized * 255)\n",
    "    \n",
    "    return frame_final\n",
    "\n",
    "# OpenCV camera capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    predicted_labels = predict_image_class(model, frame, class_mapping)\n",
    "    print(f'Predicted Labels: {predicted_labels}')\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe433d3a-b67a-4c97-a305-f589f87a52c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
